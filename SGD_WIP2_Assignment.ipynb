{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SGD WIP2 Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nayas123/Implementing-SGD-claasifier-from-scratch-without-sckitlearn/blob/main/SGD_WIP2_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eiDWcM_MC3H"
      },
      "source": [
        "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfe2NTQtLq11"
      },
      "source": [
        "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5DSPCLxqT-"
      },
      "source": [
        "<font color='red'> Importing packages</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Et8BKIxnsp"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import linear_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpSk3WQBx7TQ"
      },
      "source": [
        "<font color='red'>Creating custom dataset</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsMp0oWzx6dv"
      },
      "source": [
        "# please don't change random_state\n",
        "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
        "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
        "# make_classification is used to create custom dataset \n",
        "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8W2fg1cyGdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60c894e-aba7-4e6a-f48a-d90a81cb5b4e"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 15), (50000,))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x99RWCgpqNHw"
      },
      "source": [
        "<font color='red'>Splitting data into train and test </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kh4dBfVyJMP"
      },
      "source": [
        "#please don't change random state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gONY1YiDq7jD"
      },
      "source": [
        "\n",
        "# Standardizing the data.\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DR_YMBsyOci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23706a5d-75a3-4f51-9fe1-fe834d543eea"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((37500, 15), (37500,), (12500, 15), (12500,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW4OHswfqjHR"
      },
      "source": [
        "# <font color='red' size=5>SGD classifier</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HpvTwDHyQQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016f5a66-7c45-4003-fbe7-157d9257ce56"
      },
      "source": [
        "# alpha : float\n",
        "# Constant that multiplies the regularization term. \n",
        "\n",
        "# eta0 : double\n",
        "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
        "\n",
        "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
        "clf\n",
        "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
              "              random_state=15, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYaVyQ2lyXcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c415a7-3210-43a7-c201-43a0a00b82eb"
      },
      "source": [
        "clf.fit(X=X_train, y=y_train) # fitting our model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Epoch 1\n",
            "Norm: 0.70, NNZs: 15, Bias: -0.501317, T: 37500, Avg. loss: 0.552526\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 1.04, NNZs: 15, Bias: -0.752393, T: 75000, Avg. loss: 0.448021\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 1.26, NNZs: 15, Bias: -0.902742, T: 112500, Avg. loss: 0.415724\n",
            "Total training time: 0.02 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 1.43, NNZs: 15, Bias: -1.003816, T: 150000, Avg. loss: 0.400895\n",
            "Total training time: 0.03 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 1.55, NNZs: 15, Bias: -1.076296, T: 187500, Avg. loss: 0.392879\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 1.65, NNZs: 15, Bias: -1.131077, T: 225000, Avg. loss: 0.388094\n",
            "Total training time: 0.04 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 1.73, NNZs: 15, Bias: -1.171791, T: 262500, Avg. loss: 0.385077\n",
            "Total training time: 0.05 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 1.80, NNZs: 15, Bias: -1.203840, T: 300000, Avg. loss: 0.383074\n",
            "Total training time: 0.06 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 1.86, NNZs: 15, Bias: -1.229563, T: 337500, Avg. loss: 0.381703\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 1.90, NNZs: 15, Bias: -1.251245, T: 375000, Avg. loss: 0.380763\n",
            "Total training time: 0.07 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 1.94, NNZs: 15, Bias: -1.269044, T: 412500, Avg. loss: 0.380084\n",
            "Total training time: 0.08 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 1.98, NNZs: 15, Bias: -1.282485, T: 450000, Avg. loss: 0.379607\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 2.01, NNZs: 15, Bias: -1.294386, T: 487500, Avg. loss: 0.379251\n",
            "Total training time: 0.09 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 2.03, NNZs: 15, Bias: -1.305805, T: 525000, Avg. loss: 0.378992\n",
            "Total training time: 0.10 seconds.\n",
            "Convergence after 14 epochs took 0.10 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
              "              random_state=15, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfkVI6GyaRO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90775e79-71cf-418a-c21c-560e0f88c8a3"
      },
      "source": [
        "clf.coef_, clf.coef_.shape, clf.intercept_\n",
        "#clf.coef_ will return the weights\n",
        "#clf.coef_.shape will return the shape of weights\n",
        "#clf.intercept_ will return the intercept term"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.89007184,  0.63162363, -0.07594145,  0.63107107, -0.38434375,\n",
              "          0.93235243, -0.89573521, -0.07340522,  0.40591417,  0.4199991 ,\n",
              "          0.24722143,  0.05046199, -0.08877987,  0.54081652,  0.06643888]]),\n",
              " (1, 15),\n",
              " array([-1.30580538]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-CcGTKgsMrY"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1_8bdzitDlM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.  We will be giving you some functions, please write code in that functions only.\n",
        "\n",
        "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU2Y3-FQuJ3z"
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
        "\n",
        "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
        "\n",
        " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
        "- for each epoch:\n",
        "\n",
        "    - for each batch of data points in train: (keep batch size=1)\n",
        "\n",
        "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
        "\n",
        "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
        "\n",
        "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
        "\n",
        "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
        "\n",
        "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
        "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
        "\n",
        "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
        "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
        "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
        "        you can stop the training\n",
        "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR_HgjgS_wKu"
      },
      "source": [
        "<font color='blue'>Initialize weights </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GecwYV9fsKZ9"
      },
      "source": [
        "def initialize_weights(dim):\n",
        "    ''' In this function, we will initialize our weights and bias'''\n",
        "    #initialize the weights to zeros array of (1,dim) dimensions\n",
        "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
        "    #initialize bias to zero\n",
        "    w= np.zeros_like(dim)\n",
        "    b=0\n",
        "    return w,b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7I6uWBRsKc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9581be-1246-42e3-bb87-9fc2cb5f5fc5"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "print('w =',(w))\n",
        "print('b =',str(b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "b = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MI5SAjP9ofN"
      },
      "source": [
        "<font color='cyan'>Grader function - 1 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1llH429wG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e889d422-8cd2-4577-8fda-40d33227496a"
      },
      "source": [
        "dim=X_train[0] \n",
        "w,b = initialize_weights(dim)\n",
        "def grader_weights(w,b):\n",
        "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
        "  return True\n",
        "grader_weights(w,b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN83oMWy_5rv"
      },
      "source": [
        "<font color='blue'>Compute sigmoid </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPv4NJuxABgs"
      },
      "source": [
        "$sigmoid(z)= 1/(1+exp(-z))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAfmQF47_Sd6"
      },
      "source": [
        "def sigmoid(z):\n",
        "    ''' In this function, we will return sigmoid of z'''\n",
        "    # compute sigmoid(z) and return\n",
        "    return 1/(1+(np.exp(-z)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YrGDwg3Ae4m"
      },
      "source": [
        "<font color='cyan'>Grader function - 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_JASp_NAfK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12aa068d-4289-4016-cabc-ee58b20a9174"
      },
      "source": [
        "def grader_sigmoid(z):\n",
        "  val=sigmoid(z)\n",
        "  assert(val==0.8807970779778823)\n",
        "  return True\n",
        "grader_sigmoid(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS7JXbcrBOFF"
      },
      "source": [
        "<font color='blue'> Compute loss </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfEiS22zBVYy"
      },
      "source": [
        "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaFDgsp3sKi6"
      },
      "source": [
        "def logloss(y_true,y_pred):\n",
        "    '''In this function, we will compute log loss '''\n",
        "    sum_loss =0\n",
        "    for i in range(len(y_true)):\n",
        "      loss_i= ((y_true[i] * np.log10(y_pred[i])) + ((1-y_true[i]) * np.log10(1-y_pred[i])))\n",
        "      sum_loss += loss_i\n",
        "    loss= -1*(1/len(y_true))*sum_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H0_IqqrxLO9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs1BTXVSClBt"
      },
      "source": [
        "<font color='cyan'>Grader function - 3 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzttjvBFCuQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90af8833-2082-4f45-8f5e-2dc67b2622eb"
      },
      "source": [
        "def grader_logloss(true,pred):\n",
        "  loss=logloss(true,pred)\n",
        "  assert(loss==0.07644900402910389)\n",
        "  return True\n",
        "true=[1,1,0,1,0]\n",
        "pred=[0.9,0.8,0.1,0.8,0.2]\n",
        "grader_logloss(true,pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKTw3U4NEIMs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQabIadLCBAB"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to  'w' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMxiYKaCQgd"
      },
      "source": [
        "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMVikyuFsKo5"
      },
      "source": [
        "def gradient_dw(x,y,w,b,alpha,N):\n",
        "    '''In this function, we will compute the gardient w.r.to w '''\n",
        "    dw = (x*(y- sigmoid(np.dot(w,x)+b))- ((alpha/N)*(w)))\n",
        "    return dw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdEtLAReHEaE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUFLNqL_GER9"
      },
      "source": [
        "<font color='cyan'>Grader function - 4 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI3xD8ctGEnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d8b8db-b226-4070-9d6a-c2f358a30533"
      },
      "source": [
        "def grader_dw(x,y,w,b,alpha,N):\n",
        "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
        "  assert(np.sum(grad_dw)==2.613689585)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE8g84_GI62n"
      },
      "source": [
        "<font color='blue'>Compute gradient w.r.to 'b' </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHvTYZzZJJ_N"
      },
      "source": [
        "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUf2ft4EZp8"
      },
      "source": [
        " def gradient_db(x,y,w,b):\n",
        "     '''In this function, we will compute gradient w.r.to b '''\n",
        "     z= np.dot(w,x)\n",
        "     db = y - (sigmoid(z)+b)\n",
        "\n",
        "     return db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbcBzufVG6qk"
      },
      "source": [
        "<font color='cyan'>Grader function - 5 </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfFDKmscG5qZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8789805-c70f-4112-aee1-9f33ea7b4195"
      },
      "source": [
        "def grader_db(x,y,w,b):\n",
        "  grad_db=gradient_db(x,y,w,b)\n",
        "  assert(grad_db==-0.5)\n",
        "  return True\n",
        "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
        "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
        "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
        "grad_y=0\n",
        "grad_w,grad_b=initialize_weights(grad_x)\n",
        "alpha=0.0001\n",
        "N=len(X_train)\n",
        "grader_db(grad_x,grad_y,grad_w,grad_b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCK0jY_EOvyU"
      },
      "source": [
        "<font color='blue'> Implementing logistic regression</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmAdc5ejEZ25"
      },
      "source": [
        "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
        "    ''' In this function, we will implement logistic regression'''\n",
        "    #Here eta0 is learning rate\n",
        "    #implement the code as follows\n",
        "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
        "    w,b = initialize_weights(X_train[0])\n",
        "    # for every epoch\n",
        "    train_logloss = []\n",
        "    test_logloss = []\n",
        "    for ep in range(epochs):\n",
        "      for i in range (len(X_train)-1):\n",
        "        #compute gradient w.r.to w (call the gradient_dw() function)\n",
        "        grad_w= gradient_dw(X_train[i], y_train[i],w,b,alpha,len(X_train))\n",
        "        #compute gradient w.r.to b (call the gradient_db() function)\n",
        "        grad_b= gradient_db(X_train[i], y_train[i],w,b)\n",
        "        #update w, b from the gradient value\n",
        "        w = w + eta0 * grad_w\n",
        "        b = b + eta0 * grad_b\n",
        "    \n",
        "        # for every data point(X_train,y_train)\n",
        "        # predict the output of x_trainand X_test[for all data points] using w,b\n",
        "      train_predict =[]\n",
        "      for row in X_train:\n",
        "        a= sigmoid(np.dot(w,row)+b)\n",
        "        train_predict.append(a)\n",
        "\n",
        "        \n",
        "      test_predict=[]\n",
        "      for row_1 in X_test:\n",
        "        b= sigmoid(np.dot(w,row_1)+b)\n",
        "        test_predict.append(b)\n",
        "      \n",
        "      # store all the loss values in a list\n",
        "      train_logloss.append(logloss(y_train,train_predict))\n",
        "      test_logloss.append(logloss (y_test, test_predict))\n",
        "      print(\n",
        "            f\"For EPOCH No : {epochs} Train Loss is : {logloss(y_train, train_predict)} and Test Loss is : {logloss(y_test, test_predict)}\"\n",
        "        )       \n",
        "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
        "\n",
        "    return w,b,train_logloss,test_logloss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUquz7LFEZ6E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716417dc-e484-4f4e-8dc1-5a2b8708e217"
      },
      "source": [
        " alpha=0.0001\n",
        "eta0=0.0001\n",
        "N=len(X_train)\n",
        "epochs=50\n",
        "w,b,train_loss, test_loss=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For EPOCH No : 50 Train Loss is : 0.22374990251209398 and Test Loss is : 0.3120680618012595\n",
            "For EPOCH No : 50 Train Loss is : 0.2124291794321653 and Test Loss is : 0.29543721640057236\n",
            "For EPOCH No : 50 Train Loss is : 0.20751808957800086 and Test Loss is : 0.2880579539561253\n",
            "For EPOCH No : 50 Train Loss is : 0.20511714137907 and Test Loss is : 0.2839739659720811\n",
            "For EPOCH No : 50 Train Loss is : 0.20382325021659925 and Test Loss is : 0.28144914775362206\n",
            "For EPOCH No : 50 Train Loss is : 0.203090326997398 and Test Loss is : 0.2797824172297114\n",
            "For EPOCH No : 50 Train Loss is : 0.20266557259570375 and Test Loss is : 0.2786329849217164\n",
            "For EPOCH No : 50 Train Loss is : 0.20241861243762654 and Test Loss is : 0.2778146357934864\n",
            "For EPOCH No : 50 Train Loss is : 0.20227727506437415 and Test Loss is : 0.2772173707696684\n",
            "For EPOCH No : 50 Train Loss is : 0.20219966383187676 and Test Loss is : 0.27677256466992056\n",
            "For EPOCH No : 50 Train Loss is : 0.20216066609464906 and Test Loss is : 0.27643564007768984\n",
            "For EPOCH No : 50 Train Loss is : 0.20214491042560076 and Test Loss is : 0.27617671546057576\n",
            "For EPOCH No : 50 Train Loss is : 0.202142869775319 and Test Loss is : 0.2759752399710028\n",
            "For EPOCH No : 50 Train Loss is : 0.20214860827316383 and Test Loss is : 0.2758167692444237\n",
            "For EPOCH No : 50 Train Loss is : 0.20215843399686137 and Test Loss is : 0.2756909562954142\n",
            "For EPOCH No : 50 Train Loss is : 0.20217007260749864 and Test Loss is : 0.27559026253482377\n",
            "For EPOCH No : 50 Train Loss is : 0.20218215059805808 and Test Loss is : 0.27550911057341676\n",
            "For EPOCH No : 50 Train Loss is : 0.20219386751147614 and Test Loss is : 0.27544331580530423\n",
            "For EPOCH No : 50 Train Loss is : 0.20220478592980481 and Test Loss is : 0.2753896980912331\n",
            "For EPOCH No : 50 Train Loss is : 0.2022146960742454 and Test Loss is : 0.27534581214443093\n",
            "For EPOCH No : 50 Train Loss is : 0.2022235282729839 and Test Loss is : 0.2753097575295215\n",
            "For EPOCH No : 50 Train Loss is : 0.20223129642838275 and Test Loss is : 0.27528004289393465\n",
            "For EPOCH No : 50 Train Loss is : 0.202238061689181 and Test Loss is : 0.2752554876691004\n",
            "For EPOCH No : 50 Train Loss is : 0.20224390934105893 and Test Loss is : 0.2752351500023644\n",
            "For EPOCH No : 50 Train Loss is : 0.20224893435303878 and Test Loss is : 0.2752182732812024\n",
            "For EPOCH No : 50 Train Loss is : 0.20225323258096828 and Test Loss is : 0.2752042459935438\n",
            "For EPOCH No : 50 Train Loss is : 0.20225689564844168 and Test Loss is : 0.2751925712649009\n",
            "For EPOCH No : 50 Train Loss is : 0.20226000819554937 and Test Loss is : 0.2751828434965484\n",
            "For EPOCH No : 50 Train Loss is : 0.20226264662918164 and Test Loss is : 0.2751747302721125\n",
            "For EPOCH No : 50 Train Loss is : 0.20226487880312508 and Test Loss is : 0.2751679582150375\n",
            "For EPOCH No : 50 Train Loss is : 0.20226676425274417 and Test Loss is : 0.27516230183987667\n",
            "For EPOCH No : 50 Train Loss is : 0.2022683547401493 and Test Loss is : 0.27515757469500435\n",
            "For EPOCH No : 50 Train Loss is : 0.20226969495316202 and Test Loss is : 0.2751536222760601\n",
            "For EPOCH No : 50 Train Loss is : 0.2022708232596142 and Test Loss is : 0.2751503163201828\n",
            "For EPOCH No : 50 Train Loss is : 0.20227177245700376 and Test Loss is : 0.27514755018608444\n",
            "For EPOCH No : 50 Train Loss is : 0.20227257048279756 and Test Loss is : 0.275145235094765\n",
            "For EPOCH No : 50 Train Loss is : 0.2022732410670542 and Test Loss is : 0.275143297057239\n",
            "For EPOCH No : 50 Train Loss is : 0.20227380431953199 and Test Loss is : 0.2751416743542316\n",
            "For EPOCH No : 50 Train Loss is : 0.20227427724979363 and Test Loss is : 0.2751403154619546\n",
            "For EPOCH No : 50 Train Loss is : 0.2022746742228351 and Test Loss is : 0.27513917734020066\n",
            "For EPOCH No : 50 Train Loss is : 0.20227500735471374 and Test Loss is : 0.27513822401607707\n",
            "For EPOCH No : 50 Train Loss is : 0.20227528685382667 and Test Loss is : 0.27513742540996206\n",
            "For EPOCH No : 50 Train Loss is : 0.20227552131372747 and Test Loss is : 0.27513675636051577\n",
            "For EPOCH No : 50 Train Loss is : 0.20227571796329824 and Test Loss is : 0.2751361958138412\n",
            "For EPOCH No : 50 Train Loss is : 0.2022758828797867 and Test Loss is : 0.2751357261483084\n",
            "For EPOCH No : 50 Train Loss is : 0.20227602116967766 and Test Loss is : 0.27513533261176126\n",
            "For EPOCH No : 50 Train Loss is : 0.2022761371219698 and Test Loss is : 0.2751350028519756\n",
            "For EPOCH No : 50 Train Loss is : 0.20227623433776534 and Test Loss is : 0.2751347265246276\n",
            "For EPOCH No : 50 Train Loss is : 0.20227631583971614 and Test Loss is : 0.2751344949658015\n",
            "For EPOCH No : 50 Train Loss is : 0.20227638416431687 and Test Loss is : 0.2751343009182179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Zf_wPARlwY"
      },
      "source": [
        "<font color='red'>Goal of assignment</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3eF_VSPSH2z"
      },
      "source": [
        "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx8Rs9rfEZ1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca59717-cb43-4827-d8d0-84125a2e88c8"
      },
      "source": [
        "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
        "w-clf.coef_, b-clf.intercept_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.06875339,  0.01985547, -0.06121372, -0.026659  , -0.0961215 ,\n",
              "         -0.08964623,  0.13227654,  0.05192071, -0.01502197, -0.00344026,\n",
              "          0.00995461,  0.0016951 , -0.06571523, -0.06939874, -0.01451776]]),\n",
              " array([2.2197794]))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "230YbSgNSUrQ"
      },
      "source": [
        "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
        "\n",
        "* epoch number on X-axis\n",
        "* loss on Y-axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O6GrRt7UeCJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "17240175-2d01-4df5-a1d2-ca31f065bb07"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epoch_x = np.array([i for i in range(0, 50)])\n",
        "\n",
        "train_log_loss= np.array(train_loss)\n",
        "test_log_loss= np.array (test_loss)\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(epoch_x,train_log_loss,linewidth=3.0)\n",
        "plt.plot(epoch_x,test_log_loss,linewidth=3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves',fontsize=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss Curves')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUN8puFoEZtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074f358b-f364-455b-f14b-58824483030f"
      },
      "source": [
        "def pred(w,b, X):\n",
        "    N = len(X)\n",
        "    predict = []\n",
        "    for i in range(N):\n",
        "        z=np.dot(w,X[i])+b\n",
        "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
        "            predict.append(1)\n",
        "        else:\n",
        "            predict.append(0)\n",
        "    return np.array(predict)\n",
        "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
        "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.36776\n",
            "1.37208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k28U1xDsLIO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMokBfs3-2PY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}